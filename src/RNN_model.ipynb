{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug prediction with LSTM Recurrent Neural Networks\n",
    "In this notebook we explore the possiblity of utilizing a sequential model to detect logical bugs in source code. We are using a similar technique in Nautrual Language Processing (NLP) to sentiment analysis to analize a piece of code. The idea behind this approach is fairly simple, we look at a small snippet of code to determine whether the code is buggy or non-buggy.\n",
    "\n",
    "This notebook will cover some machine learning topics such as recurrent neural networks and long short-term memory units (LSTMs). We also breifly discuss techniques for padding squences in tensorflow. \n",
    "\n",
    "# Word Embeddings for source code\n",
    "In order to understand how deep leaning can be applied to analize source code, one popular technique is by creating word embeddings for each word in the code. To understand why we use word embeddings, we need to think about how neural networks work. For a neural network to run efficently the dot products of matrices are often used to make calculations. So, the network will be expecting scalar values, or a vector of scalar values, so it's obvious we will not be able to just feed string values into the model. We need a way to represent strings with scalar values. One approach can be to use one-hot representations to prepair the data, however this medthod doesn't give the model much information about what the word actually means.\n",
    "\n",
    "For better results we will used word embeddings, which require a machine learning method to generate, namely using a skip-gram model. Put simiply Word2Vec will process the corpus, looking at one block of text at a time to train a neural network to guess the current word given some amount of previous words. But we do not need to worry about that as we are using a premade Word2Vec model provided by SciKit Learn. \n",
    "\n",
    "(The embeddings for this project are being generated in the json_to_vector.py file of this project.)\n",
    "\n",
    "![bugAnalysis2.png](imgs/bugAnalysis2.png)\n",
    "\n",
    "Now when we do this, each example we feed into the network our input is now a (number of words in the example) x D-dimensional matrix. The dimension of the embeddings we are currently using is 100 so the above example would be of size (16,100).\n",
    "\n",
    "The vectors are generated in such a way that each dimension represents a feature and this vector will give the machine some context into what the word means and how it relates to other words.\n",
    "\n",
    "![Figure_1000.png](imgs/Figure_1000.png)\n",
    "\n",
    "You can see word that have a similar meaning are grouped together, in the example above we can see trig fuctions are grouped next to eachother as you would expect. Before we create these embeddings we are going to tokenize the source code and I will explain how and why we are doing this. The model is taking a large corpus of source code and outputs vectors for each unique word and token. We store the output into an embedding matrix.\n",
    "\n",
    "![code2emb.png](imgs/code2emb.png)\n",
    "\n",
    "# Tokenizing Source Code\n",
    "In this project we offer a simple Java tokenizer that will replace any Java syntax with token. We do this for a few reasons, one reason is beacuse it helps split the code apart and we don't have to rely on whitespace to divide the words. For example, it ensures the line \"area=5;\" will split into 4 seperate values instead of generating 1 embedding for the entire line. Another reason we did this instead of removing all syntax is it will give the model more information about how words are related to eachother. The hope is the model will be able to more easily learn logic pattern, for example, when the model sees something like \"foo(bar);\" we would like the model to learn there is some relationship between foo and bar since bar is in parentheses directly after foo, i.e. we would like the model to more easily learn bar is an argument in a call to foo.\n",
    "\n",
    "#### Photo placeholder\n",
    "\n",
    "# Generating Buggy Datasets\n",
    "In this project we also take a look at generating buggy datasets for training. With the source code tokenized we can search corpus's of code for basic logic patterns. Right now we are only generating bugs with swapped arguments. Meaning, we take a line of that has a function call with 2 arguments and simply swap the arugments so that we have an example of code that is likely working and a piece of code that is buggy.\n",
    "\n",
    "#### Photo placeholder\n",
    "\n",
    "We are only using examples of swapped arguments so that will be the bug this model will be able to detct. But this model can be scaled quite easily, all you would have to do is look into generating different logical bugs so the model can learn a wider variety of bugs. \n",
    "\n",
    "Another idea to help with generating bugs would be to implement some kind of tagging system, which was outside the scope of this project. To read more about tagging and why it would make generating bugs even easier refer to the Natural Language Toolkit's (NLTK) __[tagging documentation.](https://www.nltk.org/book/ch05.html)__\n",
    "# Recurrent Neural Networks (RNNs)\n",
    "Now that we have everything set up we are ready to jump into deep learning! In Natural Language Processing (NLP) there is a squential aspect to the data, meaning the order of the words is important. Similarily in code what comes before or after a word/syntax is very important. In order to keep track of the order of input we must utilize a squence model, a recurrent neural network.\n",
    "\n",
    "The structure of a recurrent neural network is different from a simple feedfoward neural network. In a traditional NN input is taken in all at once, the input also has a fixed size. They look something like this...\n",
    "\n",
    "![TraditionalNN.png](imgs/NN.png)\n",
    "\n",
    "The main difference in a RNN is that we now take input in a squential fasion. Each word in the input is now associated with a specific time step. \n",
    "\n",
    "![RNN.png](imgs/RNN.png)\n",
    "\n",
    "At each time step will calculate activation values that will be passed along to the next time step, often referred to as hidden states. These hidden states will contain information about what the model has already seen in previous time steps. At each step the hidden state is calculated using the current input, $x_{t}$, and the previsous hidden state, $h_{t-1}$. There is also a bias being added what was left out of this equations. The sigma below refers to an activation function which is normally tanh, and sometimes sigmoid.\n",
    "\n",
    "![HiddenState.png](imgs/HiddenState.png)\n",
    "\n",
    "The 2 W terms refer to the weight matrices. Note that the superscripts are different, it is a common notation standard to label the weight matrices with a superscript or subscript that informs you where to use that matrix. Moreover, for the $W^{X}$ matrix, the superscript X tells us this martix is assosiated with the input, detoned $x_{t}$, and we should take the dot product of the two. The same logic applies to the $W^{H}$ matrix. Both of the weight matrices are shared at each time step.\n",
    "\n",
    "The weight matrices are updated through an optimization process called backpropagation through time.\n",
    "\n",
    "At the last time step we take the dot product of the activation values and a third $W^{A}$ matrix and feed the value to a sigmoid function to get a descrete output, between 0 and 1, representing the how confident the model is the input was a buggy piece of code.\n",
    "\n",
    "#### Photo placeholder\n",
    "\n",
    "# Long Short-Term Memory Units (LSTMs)\n",
    "\n",
    "![LSTM.png](imgs/LSTM.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNLSTM, Embedding\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"clean.json\") as f:\n",
    "    clean = json.load(f)\n",
    "with open(\"buggy.json\") as f:\n",
    "    buggy = json.load(f)\n",
    "with open(\"py2vec_modelJ.json\") as f:\n",
    "    embs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = np.asarray(clean)\n",
    "buggy = np.asarray(buggy)\n",
    "buggy_labels = np.ones(len(buggy))\n",
    "clean_labels = np.zeros(len(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "safemax\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = []\n",
    "int_to_word = []\n",
    "word_to_int = {}\n",
    "i = 0\n",
    "for word, emb in embs.items():\n",
    "    embedding_matrix.append(emb)\n",
    "    int_to_word.append(word)\n",
    "    word_to_int[word] = i\n",
    "    i += 1\n",
    "    \n",
    "embedding_matrix.append(np.zeros(100))\n",
    "embedding_matrix = np.asarray(embedding_matrix)\n",
    "print(word_to_int['safemax'])\n",
    "print(int_to_word[2])\n",
    "print(np.array_equal(embs['safemax'], embedding_matrix[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate((clean, buggy), axis=0)\n",
    "train_labels = np.concatenate((clean_labels, buggy_labels), axis=0)\n",
    "\n",
    "for i in range(train_data.shape[0]):\n",
    "    string = ''\n",
    "    for j in range(len(train_data[i])):\n",
    "        string += train_data[i][j] + ' '\n",
    "    train_data[i] = string\n",
    "    \n",
    "np.random.seed(3)\n",
    "np.random.shuffle(train_data)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = train_data[train_data.shape[0]-1000:]\n",
    "test_labels = train_labels[train_labels.shape[0]-1000:]\n",
    "train_data = train_data[:train_data.shape[0]-1000]\n",
    "train_labels = train_labels[:train_labels.shape[0]-1000]\n",
    "\n",
    "num_words = len(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words embedding found 497424\n",
      "Number of words embedding missing 13988\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = []\n",
    "test_data_tokens = []\n",
    "num_words_missed = 0\n",
    "num_words_found = 0\n",
    "for i in range(train_data.shape[0]):\n",
    "    train_data_tokens.append([])\n",
    "    for word in train_data[i].split():\n",
    "        if word.lower() in embs:\n",
    "            train_data_tokens[i].append(word_to_int[word.lower()])\n",
    "            num_words_found += 1\n",
    "        else:\n",
    "            train_data_tokens[i].append(-1)\n",
    "            num_words_missed += 1\n",
    "for i in range(test_data.shape[0]):\n",
    "    test_data_tokens.append([])\n",
    "    for word in test_data[i].split():\n",
    "        if word.lower() in embs:\n",
    "            test_data_tokens[i].append(word_to_int[word.lower()])\n",
    "            num_words_found += 1\n",
    "        else:\n",
    "            train_data_tokens[i].append(embedding_matrix.shape[0]-1)\n",
    "            num_words_missed += 1\n",
    "print(\"Number of words embedding found %d\" % num_words_found)\n",
    "print(\"Number of words embedding missing %d\" % num_words_missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 5404, 3580, 2298, 2561, 3877, 4603, 1649, 5626, 4029, 994, 5934]\n",
      "[994, 435, 2561, 1728, 4603, 2170, 5626, 5626, 1937]\n",
      "5935\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokens[0])\n",
    "print(test_data_tokens[0])\n",
    "print(embedding_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2526, 2529, 4068, 4068, 5340, 994, 3984, 2561, 2905, 4603, 5918, 5626, 5934]\n",
      "_atsignsymbol_ gwtincompatible _divide_ _divide_ doublemath _dispatch_ roundtoint _openparen_ double _comma_ roundingmode _closeparen_ unknown\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokens[2])\n",
    "int_to_word.append(\"unknown\")\n",
    "def tokens_to_string(tokens):\n",
    "    words = [int_to_word[token] for token in tokens if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "print(tokens_to_string(train_data_tokens[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in train_data_tokens + test_data_tokens]\n",
    "num_tokens = np.asarray(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.09547433989306"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382613968358966"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pad = pad_sequences(train_data_tokens, maxlen=max_tokens,\n",
    "                              padding=pad, truncating=pad)\n",
    "test_data_pad = pad_sequences(test_data_tokens, maxlen=max_tokens,\n",
    "                             padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35282, 54)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,  400, 5404,\n",
       "       3580, 2298, 2561, 3877, 4603, 1649, 5626, 4029,  994, 5934],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/paperspace/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "num_words = len(int_to_word)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                   output_dim=embedding_matrix.shape[1],\n",
    "                   input_length=max_tokens,\n",
    "                   weights=[embedding_matrix],\n",
    "                   trainable=False,\n",
    "                   name='embedding_layer'))\n",
    "#model.add(Embedding(input_dim=num_words,\n",
    "#                   output_dim=100,\n",
    "#                   input_length=max_tokens,\n",
    "#                   name='embedding_layer'))\n",
    "model.add(CuDNNLSTM(16, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(8))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 54, 100)           593500    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 54, 16)            7552      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 54, 16)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 8)                 832       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 601,893\n",
      "Trainable params: 8,393\n",
      "Non-trainable params: 593,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33517 samples, validate on 1765 samples\n",
      "Epoch 1/10\n",
      "33517/33517 [==============================] - 8s 237us/step - loss: 0.6935 - acc: 0.5052 - val_loss: 0.6898 - val_acc: 0.5439\n",
      "Epoch 2/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.6155 - acc: 0.6462 - val_loss: 0.5089 - val_acc: 0.7275\n",
      "Epoch 3/10\n",
      "33517/33517 [==============================] - 6s 172us/step - loss: 0.4859 - acc: 0.7485 - val_loss: 0.4309 - val_acc: 0.7841\n",
      "Epoch 4/10\n",
      "33517/33517 [==============================] - 6s 170us/step - loss: 0.4358 - acc: 0.7776 - val_loss: 0.3932 - val_acc: 0.7977\n",
      "Epoch 5/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.4088 - acc: 0.7922 - val_loss: 0.3757 - val_acc: 0.8062\n",
      "Epoch 6/10\n",
      "33517/33517 [==============================] - 6s 166us/step - loss: 0.3913 - acc: 0.7984 - val_loss: 0.3649 - val_acc: 0.8125\n",
      "Epoch 7/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.3741 - acc: 0.8056 - val_loss: 0.3518 - val_acc: 0.8113\n",
      "Epoch 8/10\n",
      "33517/33517 [==============================] - 6s 169us/step - loss: 0.3615 - acc: 0.8129 - val_loss: 0.3363 - val_acc: 0.8278\n",
      "Epoch 9/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.3538 - acc: 0.8152 - val_loss: 0.3344 - val_acc: 0.8204\n",
      "Epoch 10/10\n",
      "33517/33517 [==============================] - 6s 169us/step - loss: 0.3416 - acc: 0.8224 - val_loss: 0.3223 - val_acc: 0.8300\n",
      "CPU times: user 1min 6s, sys: 7.63 s, total: 1min 14s\n",
      "Wall time: 59.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fead2842cf8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_data_pad, train_labels,\n",
    "         validation_split=0.05, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 152us/step\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data_pad, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 80.10%\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
